HDFS
helm install --namespace=default --set hdfs.dataNode.replicas=1 --set yarn.nodeManager.replicas=1 --set hdfs.webhdfs.enabled=true my-hadoop-cluster stable/hadoop


docker run -it --rm -v "%cd%:/data" --name=pyspark jupyter/pyspark-notebook spark-submit /data/test.py
E:\DHBW-MA\Semester 4\Big Data\Architektur

docker run -v "e:/DHBW-MA/Semester 4/Big Data/Architektur":/data ls /data

docker run -it --rm -v "C:\:/data" -p 8888:8888 jupyter/pyspark-notebook

docker run --rm -ti -p 8042:8042 -p 8088:8088 -p 19888:19888 -p 50070:50070 -p 50075:50075 --name hadoop harisekhon/hadoop

docker exec -ti hadoop bash
hdfs dfs -ls /

hdfs dfs -mkdir /data
curl https://data.humdata.org/hxlproxy/api/data-preview.csv?url=https%3A%2F%2Fraw.githubusercontent.com%2FCSSEGISandData%2FCOVID-19%2Fmaster%2Fcsse_covid_19_data%2Fcsse_covid_19_time_series%2Ftime_series_covid19_confirmed_global.csv | hdfs dfs -put -f - /data/time_series_covid19_confirmed_global.csv
hdfs dfs -chmod 777 /data

docker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' 181f11a5aadd 

docker exec 

docker stop $(docker ps -a -q)