{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start of the Program\n",
    "Import Spark Context, Spark Session and Spark Conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import expr\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import monotonically_increasing_id, row_number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a Spark session with name Spark session\n",
    "Because we are running it in local mode the master is local otherwise we need to mention Yarn or some other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"PySpark Assignment\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Specify the path where the file is Saved\n",
    "Since the hadoop cluster is running locally we need to mention the location where the namenode is running, which in this case is localhost:9000\n",
    "So the final Path will be - \n",
    "hdfs://localhost:9000/folder/filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_file_path = \"hdfs://hdfs:8020/data/time_series_covid19_confirmed_global.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read the file from the location sepcified above.\n",
    "From Spark session reading the file stored in HDFS location mentioning option \"header\" as \"true\" to read the schema as it is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileDf = spark.read.option(\"header\", \"true\").csv(hdfs_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a temp view\n",
    "Creating a temp view from the dataframe which can be used in Spark.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileDf.createOrReplaceTempView(\"number_of_cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate the number of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "numberOfColumns = len(fileDf.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List to store the last 14 days column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrayOfLast14DaysColumn = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loop through to get the latest 14 days column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 15):\n",
    "    arrayOfLast14DaysColumn.append(numberOfColumns - i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get last 14 days data into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "last14DaysDF = fileDf.select(*(fileDf.columns[i] for i in arrayOfLast14DaysColumn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Showing 5 rows out of the last 14 days dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+\n",
      "|8/31/20|8/30/20|8/29/20|8/28/20|8/27/20|8/26/20|8/25/20|8/24/20|8/23/20|8/22/20|8/21/20|8/20/20|8/19/20|8/18/20|\n",
      "+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+\n",
      "|  38165|  38162|  38143|  38140|  38129|  38113|  38070|  38054|  37999|  37953|  37894|  37856|  37599|  37599|\n",
      "|   9513|   9380|   9279|   9195|   9083|   8927|   8759|   8605|   8427|   8275|   8119|   7967|   7812|   7654|\n",
      "|  44494|  44146|  43781|  43403|  43016|  42619|  42228|  41858|  41460|  41068|  40667|  40258|  39847|  39444|\n",
      "|   1176|   1124|   1124|   1124|   1098|   1098|   1060|   1060|   1045|   1045|   1045|   1024|   1024|   1005|\n",
      "|   2654|   2624|   2551|   2471|   2415|   2332|   2283|   2222|   2171|   2134|   2068|   2044|   2015|   1966|\n",
      "+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "last14DaysDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summing up the values of the last 14 days and saving it into a column named result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultSumDf = last14DaysDF.withColumn('result', sum(last14DaysDF[col] for col in last14DaysDF.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show 5 rows out of the resultSumDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+--------+\n",
      "|8/31/20|8/30/20|8/29/20|8/28/20|8/27/20|8/26/20|8/25/20|8/24/20|8/23/20|8/22/20|8/21/20|8/20/20|8/19/20|8/18/20|  result|\n",
      "+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+--------+\n",
      "|  38165|  38162|  38143|  38140|  38129|  38113|  38070|  38054|  37999|  37953|  37894|  37856|  37599|  37599|531876.0|\n",
      "|   9513|   9380|   9279|   9195|   9083|   8927|   8759|   8605|   8427|   8275|   8119|   7967|   7812|   7654|120995.0|\n",
      "|  44494|  44146|  43781|  43403|  43016|  42619|  42228|  41858|  41460|  41068|  40667|  40258|  39847|  39444|588289.0|\n",
      "|   1176|   1124|   1124|   1124|   1098|   1098|   1060|   1060|   1045|   1045|   1045|   1024|   1024|   1005| 15052.0|\n",
      "|   2654|   2624|   2551|   2471|   2415|   2332|   2283|   2222|   2171|   2134|   2068|   2044|   2015|   1966| 31950.0|\n",
      "+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resultSumDf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the country details from the main dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "countryDF = fileDf.select(col(\"Lat\").alias(\"latitude\"), col(\"Long\").alias(\"longitude\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|latitude|longitude|\n",
      "+--------+---------+\n",
      "|33.93911|67.709953|\n",
      "| 41.1533|  20.1683|\n",
      "| 28.0339|   1.6596|\n",
      "| 42.5063|   1.5218|\n",
      "|-11.2027|  17.8739|\n",
      "+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "countryDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add a row index to resultSumDf and countryDF, so that we can Join the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultSumDfRowIndex=resultSumDf.withColumn('row_index', row_number().over(Window.orderBy(monotonically_increasing_id())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "countryDFRowIndex = countryDF.withColumn('row_index', row_number().over(Window.orderBy(monotonically_increasing_id())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joining the dataframes to get the final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "countryDFWithSum = resultSumDfRowIndex.join(countryDFRowIndex, on=[\"row_index\"]).drop(\"row_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show 5 rows from the countryDFWithSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+--------+--------+---------+\n",
      "|8/31/20|8/30/20|8/29/20|8/28/20|8/27/20|8/26/20|8/25/20|8/24/20|8/23/20|8/22/20|8/21/20|8/20/20|8/19/20|8/18/20|  result|latitude|longitude|\n",
      "+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+--------+--------+---------+\n",
      "|  38165|  38162|  38143|  38140|  38129|  38113|  38070|  38054|  37999|  37953|  37894|  37856|  37599|  37599|531876.0|33.93911|67.709953|\n",
      "|   9513|   9380|   9279|   9195|   9083|   8927|   8759|   8605|   8427|   8275|   8119|   7967|   7812|   7654|120995.0| 41.1533|  20.1683|\n",
      "|  44494|  44146|  43781|  43403|  43016|  42619|  42228|  41858|  41460|  41068|  40667|  40258|  39847|  39444|588289.0| 28.0339|   1.6596|\n",
      "|   1176|   1124|   1124|   1124|   1098|   1098|   1060|   1060|   1045|   1045|   1045|   1024|   1024|   1005| 15052.0| 42.5063|   1.5218|\n",
      "|   2654|   2624|   2551|   2471|   2415|   2332|   2283|   2222|   2171|   2134|   2068|   2044|   2015|   1966| 31950.0|-11.2027|  17.8739|\n",
      "+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+--------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "countryDFWithSum.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "countryDFWithSum.createOrReplaceTempView(\"countryDFWithSum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group by the country name to sum up all the result for a particular country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "numberOfCasesPerCountry = spark.sql(\"select latitude, longitude, sum(result) as total_numner_of_cases from countryDFWithSum group by latitude, longitude\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The directory path where we need to store the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultLocation = \"hdfs://hdfs:8020/data/result\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store the final DF to the result folder as a csv file here coalesce is used to have just one single file instead of partitioned file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "numberOfCasesPerCountry.coalesce(1).write.mode('overwrite').option(\"header\", \"true\").csv(resultLocation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write the dataframe into the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "numberOfCasesPerCountry.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost:5432/stephan_spark\") \\\n",
    "    .option(\"dbtable\", \"number_of_cases\") \\\n",
    "    .option(\"user\", \"postgres\") \\\n",
    "    .option(\"password\", \"root\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the data from the table to verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_query_result = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost:5432/stephan_spark\") \\\n",
    "    .option(\"dbtable\", \"number_of_cases\") \\\n",
    "    .option(\"user\", \"postgres\") \\\n",
    "    .option(\"password\", \"root\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    ".load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show the result from the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+---------------------+---------+\n",
      "| latitude| longitude|total_numner_of_cases|row_index|\n",
      "+---------+----------+---------------------+---------+\n",
      "| 33.93911| 67.709953|             531310.0|        1|\n",
      "|  41.1533|   20.1683|             118981.0|        2|\n",
      "|  28.0339|    1.6596|             582820.0|        3|\n",
      "|  42.5063|    1.5218|              14881.0|        4|\n",
      "| -11.2027|   17.8739|              31231.0|        5|\n",
      "|  17.0608|  -61.7964|               1314.0|        6|\n",
      "| -38.4161|  -63.6167|            4909293.0|        7|\n",
      "|  40.0691|   45.0382|             598732.0|        8|\n",
      "| -35.4735|  149.0124|               1582.0|        9|\n",
      "| -33.8688|  151.2093|              55958.0|       10|\n",
      "| -12.4634|  130.8456|                462.0|       11|\n",
      "| -27.4698|  153.0251|              15480.0|       12|\n",
      "| -34.9285|  138.6007|               6476.0|       13|\n",
      "| -42.8821|  147.3272|               3220.0|       14|\n",
      "| -37.8136|  144.9631|             256415.0|       15|\n",
      "| -31.9505|  115.8605|               9130.0|       16|\n",
      "|  47.5162|   14.5501|             355291.0|       17|\n",
      "|  40.1431|   47.5769|             494501.0|       18|\n",
      "|25.025885|-78.035889|              24484.0|       19|\n",
      "|  26.0275|     50.55|             693070.0|       20|\n",
      "+---------+----------+---------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "database_query_result.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
